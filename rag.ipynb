{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74c7e804",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade langchain langchain-community langchain-chroma langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54960405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a25f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "with open('creds.json') as file:\n",
    "  creds = json.load(file)\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = creds[\"OPENAI_API_KEY\"]\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = creds[\"LANGCHAIN_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e186aa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca105f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "# from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Завантажуємо, розбиваємо на частини та індексуємо вміст блогу.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298946a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отримуємо та генеруємо, використовуючи відповідні фрагменти блогу.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39082a4",
   "metadata": {},
   "source": [
    "Detailed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49750a29",
   "metadata": {},
   "source": [
    "Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75d306f",
   "metadata": {},
   "source": [
    "Load from web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28a0d75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43047"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Лишаємо лише назву, заголовки та вміст допису з повного HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2edbf28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52f38e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, list, langchain_core.documents.base.Document)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs), type(docs), type(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f2ef04",
   "metadata": {},
   "source": [
    "Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f8470b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e17e060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "969"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "513a34a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 8436}, page_content='The paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "387b8290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[10].page_content[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "845d05ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'start_index': 8436}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[10].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9c4648",
   "metadata": {},
   "source": [
    "Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdd4d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab100fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x27edc4afbc0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f57c00",
   "metadata": {},
   "source": [
    "Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90da6791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")\n",
    "\n",
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd1b3cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n",
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b478257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Resources:\n",
      "1. Internet access for searches and information gathering.\n",
      "2. Long Term memory management.\n",
      "3. GPT-3.5 powered Agents for delegation of simple tasks.\n",
      "4. File output.\n",
      "\n",
      "Performance Evaluation:\n",
      "1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n",
      "2. Constructively self-criticize your big-picture behavior constantly.\n",
      "3. Reflect on past decisions and strategies to refine your approach.\n",
      "4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.' metadata={'start_index': 29546, 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ec679c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langsmith in c:\\users\\asus\\anaconda3\\envs\\langchain\\lib\\site-packages (0.4.38)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\asus\\anaconda3\\envs\\langchain\\lib\\site-packages (from langsmith) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\asus\\anaconda3\\envs\\langchain\\lib\\site-packages (from langsmith) (3.11.4)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\asus\\anaconda3\\envs\\langchain\\lib\\site-packages (from langsmith) (25.0)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\asus\\anaconda3\\envs\\langchain\\lib\\site-packages (from langsmith) (2.12.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\asus\\anaconda3\\envs\\langchain\\lib\\site-packages (from langsmith) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\asus\\anaconda3\\envs\\langchain\\lib\\site-packages (from langsmith) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\asus\\anaconda3\\envs\\langchain\\lib\\site-packages (from langsmith) (0.25.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\asus\\anaconda3\\envs\\langchain\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\asus\\anaconda3\\envs\\langchain\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\asus\\anaconda3\\envs\\langchain\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\asus\\anaconda3\\envs\\langchain\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\asus\\anaconda3\\envs\\langchain\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\asus\\anaconda3\\envs\\langchain\\lib\\site-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\asus\\anaconda3\\envs\\langchain\\lib\\site-packages (from pydantic<3,>=1->langsmith) (2.41.4)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\asus\\anaconda3\\envs\\langchain\\lib\\site-packages (from pydantic<3,>=1->langsmith) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\asus\\anaconda3\\envs\\langchain\\lib\\site-packages (from pydantic<3,>=1->langsmith) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\envs\\langchain\\lib\\site-packages (from requests>=2.0.0->langsmith) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\envs\\langchain\\lib\\site-packages (from requests>=2.0.0->langsmith) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\asus\\anaconda3\\envs\\langchain\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langsmith\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f601d590",
   "metadata": {},
   "source": [
    "Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "156c3006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "prompt = client.pull_prompt(\"rlm/rag-prompt\")  # Replace with your prompt ID\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62f9687d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: {question} \n",
      "Context: {context} \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(prompt.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0873b315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Decomposition is a process of breaking down complicated tasks into simpler, more manageable steps. The aim is to enhance performance on complex tasks by directing the model to operate 'step by step'. The process can explore multiple reasoning possibilities at each step, creating a structure similar to a tree. Techniques or methods for task decomposition can include simple prompting, task-specific instructions, or human input."
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in rag_chain.stream(\"What is Task Decomposition?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4072bdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is a process of breaking down complex tasks into smaller, simpler steps. This technique is used to enhance performance on complex tasks as it transforms big tasks into multiple manageable tasks. Different methods can be used to achieve this, such as using simple prompts, task-specific instructions, human inputs, or relying on an external classical planner for long-horizon planning.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = rag_chain.invoke(\"What is Task Decomposition?\")\n",
    "resp\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
